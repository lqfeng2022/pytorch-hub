export default [
  { id: 0,
    name: "Chapter 0: Artificial Intelligence",
    quote: "A computer would deserve to be called intelligent if it could deceive a human into believing that it was human.", 
    author: "― Alan Turing",
    image: "covers/cover_0.webp",
    title: "Here's the ChatGPT introduction from OpenAI homepage.", 
    description: "When ChatGPT-3.5 came out on November 30, 2022, it was the first time that AI impressed us by its human-like text. It can help with answering questions, writing essays, creating content, tutoring in various subjects, and even having casual conversations. Each version has improved in terms of understanding context, handling more complex queries, and generating more accurate and relevant responses. In short, ChatGPT is like having a smart, versatile assistant right at your fingertips."
  },
  { id: 1,
    name: "Chapter 1: TENSORs",
    quote: "Maxwell's equations… when rewritten using time as the fourth dimension, this rather awkward set of eight equations collapses into a single tensor equation. This is what a physicist calls 'beauty'.", 
    author: "― Michio Kaku, a physicist",
    image: "covers/tesseract.jpg",
    title: "Model of A Tesseract (or hypercube), in an otherworldly setting | Robert Brook/Science Photo Library Art", 
    description: "This image represents a tesseract, a 4-dimensional hypercube. While we can't see the 4th dimension, this image gives us a 3D shadow. You can image a 4D flashlight shining onto our 3D space, and the tesseract positioned in the middle. similar to how a light casts a shadow of a 3D cube onto a 2D surface—resulting in two squares, one inside the other. In this image, the tesseract appears as a cube within a cube, connected at the corners. In 4D space, both cubes are actually the same size, with the connecting lines representing higher-dimensional relationships."
  },
  { id: 2,
    name: "Chapter 2: A Line Model",
    quote: "The straight line belongs to men, the curved one to God.", 
    author: "― Antoni Gaudi, an architect and designer",
    image: "covers/straight_lines.jpg",
    title: "'How To Draw Freakishly Straight Lines by Hand' - 'tatyanadeniz.com/straight-lines'", 
    description: "This drawing is all about straight lines creating a cool, spiraling pattern. It’s like a bunch of triangles and angles that pull you in towards the center. Even though it’s just straight lines, they come together in a way that feels almost like it’s moving or shifting as you look at it. It’s a great example of how straight lines can be used to create something that feels dynamic and alive, despite their rigid and orderly nature."
  },
  { id: 3,
    name: "Chapter 3: The Maths Behind (I)",
    quote: "Gradient descent can write code better than you. I'm sorry.",
    author: "― Andrej Karpathy, Co-founded OpenAI",
    image: "covers/gradient_descent.jpeg",
    title: "GRADIENT DESCENT SIMULATION - TWO PARAMETERS",
    description: "This graph is a powerful visualization of how Gradient Descent operates when two parameters are involved. It shows the path the algorithm takes across the parameter space (θ₁ and θ₂) to minimize the loss function - Loss(θ₁,θ₂). The movement along the surface illustrates how the parameters are iteratively adjusted to reduce the loss, eventually converging to a point where the model is optimally trained."
  },
  { id: 4,
    name: "Chapter 4: A Classification Model",
    quote: "At its core, machine learning is about learning to make predictions on the basis of data. It’s fundamentally about generalization, about forming rules that enable us to classify unseen examples.",
    author: "― Pedro Domingos, 'The Master Algorithm'",
    image: "covers/classification.webp",
    title: "Visualizing the Process of Data Classification: From Input to Output",
    description: "This image acts as a visual metaphor for how machine learning models, particularly neural networks, process input data to generate classified or simplified outputs. It illustrates the transformation of raw data into meaningful information through computational processes, which is fundamental to the operation of machine learning systems."
  },
  { id: 5,
    name: "Chapter 5: The Maths Behind (II)",
    quote: "The backpropagation algorithm is possibly the most important algorithm in deep learning.",
    author: "― John D. Kelleher, 'Deep Learning'",
    image: "covers/backpropagation.webp",
    title: "Multilayer Perceptron architecture and our notation. Handmade sketch by D Goglia.",
    description: "This handmade sketch display a pretty basic architecture in deep learing, which we call it multiplayer perceptron (MLP), we'll cover it in our future model. Here this neural network is pretty simple, which has only one hidden layer with fully connected feed-forward. In last chapter, we built a neural network with this kind of architecure, here in this chapter, we'll talk about the backpropagation, - a core algrithm behind MLP."
  },
  { id: 6,
    name: "Chapter 6: A CNN Model",
    quote: "A baby learns to see before it learns to speak. The development of vision capabilities is a prerequisite for developing general intelligence in machines.",
    author: "― Yann LeCun, known for CNNs",
    image: "covers/generated_girl.webp",
    title: "Closeup beauty of the computer generated girl. CGI portrait render, 3D rendering, by Creative Photo Corner",
    description: "Computer vision has advanced to the point where AI can generate hyper-realistic, lifelike images, like this beautiful computer-generated face that’s almost indistinguishable from reality. These developments are transforming how we interact with technology, making virtual assistants and bots feel more natural and engaging. Beyond technology, they challenge our perceptions of identity, authenticity, and reality in the digital age."
  },
  { id: 7,
    name: "Chapter 7: The Maths Behind (III)",
    quote: "AI will be the most transformative technology of the 21st century. It will affect every industry and aspect of our lives.",
    author: "― Jensen Huang, Co-founding of Nvidia",
    image: "covers/naushika.jpg",
    title: "Nausicaä from Nausicaä of the Valley of the Wind (1984) by books_baking_broadway",
    description: "The image on the left portrays a non-animated, lifelike version of Nausicaä, the Studio Ghibli character from *Nausicaä of the Valley of the Wind* (1984), generated by an AI model. Using advanced computer vision techniques like image synthesis, AI can transform animated characters into realistic forms by learning from vast datasets of real-world images and animation styles. This showcases how AI-driven tools, such as GANs, are able to reinterpret 2D animations into photorealistic depictions, highlighting the potential of AI in expanding the ways we visualize and engage with animated media."
  },
  { id: 8,
    name: "Chapter 8: A Vision Transformer Model",
    quote: "When eyes were first developed in animals, suddenly animal life becomes proactive.",
    author: "― Fei-Fei Li, Establishing ImageNet",
    image: "covers/patch_embedding.jpg",
    title: "Personalised Word Art Portrait, Anniversary Art Print by Scribbly Goose",
    description: "This is a wedding photo composed of text, where the variations in light and shadow of the text outline the entire image, and the words themselves narrate the couple’s story, making it very creative. The idea of arranging text to form a picture is quite intriguing. Inspired by this, could we in the field of computer vision break down an image into individual 'text-like' units and then encode and analyze these 'text' elements in a manner similar to natural language processing?"
  },
  { id: 9,
    name: "Chapter 9: The Maths Behind (IV)",
    quote: "If you value intelligence above all other human qualities, you're gonna have a bad time.",
    author: "― Ilya Sutskever, former chief scientist at OpenAI",
    image: "covers/yamatanoorochi.jpeg",
    title: "Yamata no Orochi Slaying - Illustration Material",
    description: "The image depicts a scene from the famous Japanese myth of Yamata no Orochi, the eight-headed serpent. In the image, we see a warrior standing ready with a sword, facing the massive serpent with multiple dragon-like heads emerging from the background, each with distinct colors and shapes. It seems that, having multiple heads means power, which coincidentally aligns with the concept of multi-head attention in Transformer models."
  },
  { id: 10,
    name: "Chapter 10: A Language Translation Model",
    quote: "AI will be the best or worst thing ever for humanity.",
    author: "― Elon Musk",
    image: "covers/languages.webp",
    title: "Natural Languages",
    description: "This vibrant image beautifully represents the diversity of human natural languages, showcasing the word “Hello” in many different languages and scripts, such as English, Spanish, Japanese, Arabic, and more. Each language comes with its unique way of expressing common ideas, reflecting cultural and linguistic variety across the globe. The image highlights the universality of human communication, while also celebrating the uniqueness of individual languages. As we move into building language translation models, it’s this rich diversity that we aim to bridge, enabling machines to understand and translate between these varied systems of communication, connecting people across linguistic barriers."
  },
  { id: 11,
    name: "Chapter 11: The Maths Behind (V)",
    quote: "ATTENTION IS ALL YOU NEED.",
    author: "― 'Transformer' paper in 2017",
    image: "covers/llms.jpg",
    title: "What are Large Language Models (LLMs)?",
    description: "Large Language Models (LLMs), like ChatGPT, have transformed the field of natural language processing by enabling machines to generate, understand, and interact using human language at an unprecedented scale. LLMs are trained on massive amounts of text data, allowing them to learn the intricate patterns, grammar, and semantics of languages. At the heart of these models is the Transformer architecture, which is pivotal to their performance. The Transformer’s key innovations, such as self-attention and parallel processing, allow LLMs to capture long-range dependencies, understand context better, and process large volumes of data efficiently. Without the Transformer, modern LLMs would struggle with the complexity and scale of tasks they now perform, from translation and summarization to conversation and creative writing."
  },
]