export default [
  { id: 0,
    name: "Chapter 0: Artificial Intelligence",
    quote: "A computer would deserve to be called intelligent if it could deceive a human into believing that it was human.", 
    author: "― Alan Turing wrote in 1950",
    image: "src/assets/cover_0.webp",
    title: "Here's the ChatGPT introduction from OpenAI homepage.", 
    description: "When ChatGPT-3.5 came out on November 30, 2022, it was the first time that AI impressed us by its human-like text. It can help with answering questions, writing essays, creating content, tutoring in various subjects, and even having casual conversations. Each version has improved in terms of understanding context, handling more complex queries, and generating more accurate and relevant responses. In short, ChatGPT is like having a smart, versatile assistant right at your fingertips."
  },
  { id: 1,
    name: "Chapter 1: TENSORs",
    quote: "Maxwell's equations… when rewritten using time as the fourth dimension, this rather awkward set of eight equations collapses into a single tensor equation. This is what a physicist calls 'beauty'.", 
    author: "― Michio Kaku",
    image: "src/assets/tesseract.jpg",
    title: "Model of A Tesseract (or hypercube), in an otherworldly setting | Robert Brook/Science Photo Library Art", 
    description: "This image represents a tesseract, a 4-dimensional hypercube. While we can't see the 4th dimension, this image gives us a 3D shadow. You can image a 4D flashlight shining onto our 3D space, and the tesseract positioned in the middle. similar to how a light casts a shadow of a 3D cube onto a 2D surface—resulting in two squares, one inside the other. In this image, the tesseract appears as a cube within a cube, connected at the corners. In 4D space, both cubes are actually the same size, with the connecting lines representing higher-dimensional relationships."
  },
  { id: 2,
    name: "Chapter 2: A Line Model",
    quote: "The straight line belongs to men, the curved one to God.", 
    author: "― Antoni Gaudi",
    image: "src/assets/straight_lines.jpg",
    title: "'How To Draw Freakishly Straight Lines by Hand' - 'tatyanadeniz.com/straight-lines'", 
    description: "This drawing is all about straight lines creating a cool, spiraling pattern. It’s like a bunch of triangles and angles that pull you in towards the center. Even though it’s just straight lines, they come together in a way that feels almost like it’s moving or shifting as you look at it. It’s a great example of how straight lines can be used to create something that feels dynamic and alive, despite their rigid and orderly nature."
  },
  { id: 3,
    name: "Chapter 3: The Maths Behind (I)",
    quote: "Gradient descent can write code better than you. I'm sorry.",
    author: "― Andrej Karpathy",
    image: "src/assets/gradient_descent.jpeg",
    title: "GRADIENT DESCENT SIMULATION - TWO PARAMETERS",
    description: "This graph is a powerful visualization of how Gradient Descent operates when two parameters are involved. It shows the path the algorithm takes across the parameter space (θ₁ and θ₂) to minimize the loss function - Loss(θ₁,θ₂). The movement along the surface illustrates how the parameters are iteratively adjusted to reduce the loss, eventually converging to a point where the model is optimally trained."
  },
  { id: 4,
    name: "Chapter 4: A Classification Model",
    quote: "At its core, machine learning is about learning to make predictions on the basis of data. It’s fundamentally about generalization, about forming rules that enable us to classify unseen examples.",
    author: "― Pedro Domingos’ book “The Master Algorithm” (2015)",
    image: "src/assets/classification.webp",
    title: "GRADIENT DESCENT SIMULATION - TWO PARAMETERS",
    description: "This image acts as a visual metaphor for how machine learning models, particularly neural networks, process input data to generate classified or simplified outputs. It illustrates the transformation of raw data into meaningful information through computational processes, which is fundamental to the operation of machine learning systems."
  },
  { id: 5,
    name: "Chapter 5: The Maths Behind (II)",
    quote: "The backpropagation algorithm is possibly the most important algorithm in deep learning.",
    author: "― John D. Kelleher, Deep Learning",
    image: "src/assets/backpropagation.webp",
    title: "Multilayer Perceptron architecture and our notation. Handmade sketch by D Goglia.",
    description: "This handmade sketch display a pretty basic architecture in deep learing, which we call it multiplayer perceptron (MLP), we'll cover it in our future model. Here this neural network is pretty simple, which has only one hidden layer with fully connected feed-forward. In last chapter, we built a neural network with this kind of simple architecure, here in this chapter, we'll talk about it deeper, especially the backpropagation, - a core algrithm behind neural networ."
  },
  { id: 6,
    name: "Chapter 6: A CNN Model",
    quote: "A baby learns to see before it learns to speak. The development of vision capabilities is a prerequisite for developing general intelligence in machines.",
    author: "― Yann LeCun",
    image: "src/assets/generated_girl.webp",
    title: "Closeup beauty of the computer generated girl. CGI portrait render, 3D rendering, by Creative Photo Corner",
    description: "Computer vision has advanced to the point where AI can generate hyper-realistic, lifelike images, like this beautiful computer-generated face that’s almost indistinguishable from reality. These developments are transforming how we interact with technology, making virtual assistants and bots feel more natural and engaging. Beyond technology, they challenge our perceptions of identity, authenticity, and reality in the digital age."
  },
  { id: 7,
    name: "Chapter 7: The Maths Behind (III)",
    quote: "AI will be the most transformative technology of the 21st century. It will affect every industry and aspect of our lives.",
    author: "― Jensen Huang",
    image: "src/assets/naushika.jpg",
    title: "Nausicaä from Nausicaä of the Valley of the Wind (1984) by books_baking_broadway",
    description: "The image on the left portrays a non-animated, lifelike version of Nausicaä, the Studio Ghibli character from *Nausicaä of the Valley of the Wind* (1984), generated by an AI model. Using advanced computer vision techniques like image synthesis, AI can transform animated characters into realistic forms by learning from vast datasets of real-world images and animation styles. This showcases how AI-driven tools, such as GANs, are able to reinterpret 2D animations into photorealistic depictions, highlighting the potential of AI in expanding the ways we visualize and engage with animated media."
  },
  { id: 8,
    name: "Chapter 8: A Vision Transformer Model",
    quote: "When eyes were first developed in animals, suddenly animal life becomes proactive.",
    author: "― Fei-Fei Li",
    image: "src/assets/patch_embedding.jpeg",
    title: "AN IMAGE IS WORTH 16X16 WORDS",
    description: "This image illustrates the patch embedding process in a Vision Transformer (ViT) model, which we’ll be constructing in this chapter. As the title suggests, ‘An Image is Worth 16x16 Words,’ we treat the cat image as a series of 16x16 pixel patches, similar to how words are processed in natural language models. These patches are then fed into a Transformer encoder, which learns to recognize patterns and features in the image, enabling the model to understand and classify visual data just like it would with sequential text data."
  },
  { id: 9,
    name: "Chapter 9: The Maths Behind (IV)",
    quote: "If you value intelligence above all other human qualities, you're gonna have a bad time.",
    author: "― Ilya Sutskever",
    image: "src/assets/mha.jpeg",
    title: "Multi-Head Attention",
    description: "Multi-Head Attention is a core component of the Transformer architecture, and it plays a crucial role in its ability to capture complex relationships within data. Multi-Head Attention provides the model with a more nuanced understanding of the input, making the Transformer architecture powerful for both sequential data (text) and spatial data (images, through Vision Transformers)."
  },
  { id: 10,
    name: "Chapter 10: A Language Translation Model",
    quote: "AI will be the best or worst thing ever for humanity.",
    author: "― Elon Musk",
    image: "src/assets/languages.webp",
    title: "Natural Languages",
    description: "This vibrant image beautifully represents the diversity of human natural languages, showcasing the word “Hello” in many different languages and scripts, such as English, Spanish, Japanese, Arabic, and more. Each language comes with its unique way of expressing common ideas, reflecting cultural and linguistic variety across the globe. The image highlights the universality of human communication, while also celebrating the uniqueness of individual languages. As we move into building language translation models, it’s this rich diversity that we aim to bridge, enabling machines to understand and translate between these varied systems of communication, connecting people across linguistic barriers."
  },
  { id: 11,
    name: "Chapter 11: The Maths Behind (V)",
    quote: "ATTENTION IS ALL YOU NEED.",
    author: "― pager in 2017",
    image: "src/assets/llms.jpg",
    title: "What are Large Language Models (LLMs)?",
    description: "Large Language Models (LLMs), like ChatGPT, have transformed the field of natural language processing by enabling machines to generate, understand, and interact using human language at an unprecedented scale. LLMs are trained on massive amounts of text data, allowing them to learn the intricate patterns, grammar, and semantics of languages. At the heart of these models is the Transformer architecture, which is pivotal to their performance. The Transformer’s key innovations, such as self-attention and parallel processing, allow LLMs to capture long-range dependencies, understand context better, and process large volumes of data efficiently. Without the Transformer, modern LLMs would struggle with the complexity and scale of tasks they now perform, from translation and summarization to conversation and creative writing."
  },
]